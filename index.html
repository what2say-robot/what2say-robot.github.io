<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FSSR6942QL"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FSSR6942QL');
    </script>

    <title>What2Say: Efficient LLM-Based Communication with Conformal Prediction for Multi-Agent Exploration</title>
    <link rel="icon" type="image/png" href="img/favicon_2.png">

    <meta name="description" content="SoNIC: Safe Social Navigation with Adaptive Conformal Inference and Constrained Reinforcement Learning">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
    <link rel="stylesheet" href="css/app.css">
    <!-- <link rel="stylesheet" href="css/bulma.min.css"> -->
    <!-- <link rel="stylesheet" href="css/bulma-carousel.min.css"> -->
    <!-- <link rel="stylesheet" href="css/bulma-slider.min.css"> -->
    <link rel="stylesheet" href="css/bulma-myscope.css">

    <!-- https://github.com/jgthms/bulma/issues/302#issuecomment-441890938 Follow this to use bulma style here without conflicts with app.css. I put bulma.min.css, bulma-carousel.min.css, and bulma-slider.min.css into a single file before applying less. -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/bulma-carousel.min.js"></script>
    <script src="js/bulma-slider.min.js"></script>
    <script src="js/index.js"></script>
    <script src="js/app.js"></script>

    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>
<body>
    <div class="container" id="main">
        <div class="row mt-4">
            <h2 class="col-md-12 text-center">
                <!-- <span style="font-weight: 900;">SoNIC</span>: </br> Safe <span style="font-weight: 900;">So</span>cial <span style="font-weight: 900;">N</span>avigation with Adaptive Conformal <span style="font-weight: 900;">I</span>nference and <span style="font-weight: 900;">C</span>onstrained Reinforcement Learning</br> -->
                </br>What2Say: Efficient LLM-Based Communication with Conformal Prediction for Multi-Agent Exploration</br>
            </h2>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <!-- <br> -->
                <!-- <li>Zejing Wang</li>
                <li><a href="//xiaopanz.github.io/">Xiaopan Zhang</a></li>
                <li><a href="https://github.com/ZhixuLi-the-Runner">Zhixu Li</a></li>
                <li><a href="//jyao97.github.io">Jianpeng Yao</a></li>
                <li><a href="https://jiachenli94.github.io/">Jiachen Li</a></li> -->
                <!-- <br> -->
                <br>
                <br>
                    <a href="https://robotics.ucr.edu//">
                        <image src="img/UCR_logo.svg" height="50px"> 
                    </a>
                </ul>
            </div>
        </div>
        
        <div class="row justify-content-md-center">
            <div class="col-md-2 text-center">
                <!-- <a href="https://arxiv.org/pdf/2407.17460"> -->
                    <img src="img/paper_img.png" height="45px">
                    <h4><strong>Paper (coming soon)</strong></h4>
                </a>
            </div>
            <div class="col-md-2 text-center">
                <!-- <a href="https://www.youtube.com/watch?v=uXqPMaSyDek"> -->
                    <img src="img/youtube_icon.png" height="45px">
                    <h4><strong>Video (coming soon)</strong></h4>
                </a>
            </div>
            <div class="col-md-2 text-center">
                <!-- <a href="https://github.com/tasl-lab/SoNIC/tree/main"> -->
                    <img src="img/github.png" height="45px">
                    <h4><strong>Code (coming soon)</strong></h4>
                </a>
            </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <br>
                <h5 class="text-justify text-center">
                    A novel LLM-Based communication framework that employs conformal prediction to evaluate the reability of LLM-generated output, achieving remakable efficiency improvement in emboided multi-agent exploration tasks.
                </h5>
                <br>
            </div>
        </div>

        <!-- <div class='myscope'>
          <section class="hero is-light is-small">
            <div class="hero-body">
              <div class="container">
                <div id="results-carousel" class="carousel results-carousel">

                  <div class="item-0">
                    <video poster="" id="0" autoplay controls muted loop playsinline height="100%">
                        <source src="videos/Case1-Ours.mov" type="video/mov">
                    </video>
                  </div>
                  <div class="item-1">
                    <video poster="" id="1" autoplay controls muted loop playsinline height="100%">
                        <source src="videos/Case2-Ours.mov" type="video/mov">
                    </video>
                  </div>
                  <!-- <div class="item-2">
                    <video poster="" id="2" autoplay controls muted loop playsinline height="100%">
                        <source src="videos/SoNIC_with_rushing_1.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="item-3">
                    <video poster="" id="4" autoplay controls muted loop playsinline height="100%">
                        <source src="videos/SoNIC_with_SF.mp4" type="video/mp4">
                    </video>
                  </div> -->

                <!-- </div>
              </div>
            </div>
          </section>
        </div> --> -->


        <!-- <br>
        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <h5 class="text-justify text-center">
                    What2Say Performance in Different Scenarios
                </h5>
            </div>
        </div>
        <br> -->

        <div class='myscope'>
            <section class="hero is-light is-small">
              <div class="hero-body">
                <div class="container">
                  <div class="video-grid">
          
                    <!-- Á¨¨‰∏ÄË°å -->
                    <div class="video-item">
                      <video poster="" autoplay controls muted loop playsinline>
                        <source src="videos/Case1-No-Com.mov" type="video/quicktime">
                      </video>
                      <p class="caption">Case 1 - without Communication</p>
                    </div>
                    <div class="video-item">
                      <video poster="" autoplay controls muted loop playsinline>
                        <source src="videos/Case1-No-CP.mov" type="video/quicktime">
                      </video>
                      <p class="caption">Case 1 - without Conformal Prediction</p>
                    </div>
                    <div class="video-item">
                      <video poster="" autoplay controls muted loop playsinline>
                        <source src="videos/Case1-Ours.mov" type="video/quicktime">
                      </video>
                      <p class="caption">Case 1 - Ours</p>
                    </div>
          
                    <!-- Á¨¨‰∫åË°å -->
                    <div class="video-item">
                      <video poster="" autoplay controls muted loop playsinline>
                        <source src="videos/Case2-No-Com.mov" type="video/quicktime">
                      </video>
                      <p class="caption">Case 2 - without Communication</p>
                    </div>
                    <div class="video-item">
                      <video poster="" autoplay controls muted loop playsinline>
                        <source src="videos/Case2-No-Takeover.mov" type="video/quicktime">
                      </video>
                      <p class="caption">Case 2 - without Takeover</p>
                    </div>
                    <div class="video-item">
                      <video poster="" autoplay controls muted loop playsinline>
                        <source src="videos/Case2-Ours.mov" type="video/quicktime">
                      </video>
                      <p class="caption">Case 2 - Ours</p>
                    </div>
          
                  </div>
                </div>
              </div>
            </section>
          </div>
          
          <style>
            .video-grid {
              display: grid;
              grid-template-columns: repeat(3, 1fr);
              gap: 20px;
            }
            
            .video-item {
              text-align: center;
            }
            
            .video-item video {
              width: 100%;
              height: auto;
            }
            
            .caption {
              margin-top: 10px;
              font-size: 14px;
              color: #333;
            }
          </style>
          
          <br>
          
          <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
              <h5 class="text-justify text-center">
                What2Say Performance in Different Scenarios
              </h5>
            </div>
          </div>
          
          <br>

        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <h3 class="mt-4 mb-2">
                    Abstract
                </h3>
                <p class="text-justify">
                    <!-- While previous research either relies on a centralized system with shared observation or does not achieve efficient communication, we address the issue of low LLM-based communication efficiency in decentralized multi-agent systems. We utilize Conformal Prediction to access the reliability of the LLM-generated information for communication and make the communication effective. To evaluate our framework, we employ embodied question answering, which involves the exploration of agents in unknown environments. For our communication setting, we introduce a new multi-agent multi-task dataset with diverse, realistic human-robot scenarios and scenes built upon the Habitat-Matterport 3D. The experiments in simulations show our proposed approach improves both the success rate and efficiency over baselines that either do not leverage communication or do not calibrate confidence.     -->
                    Embodied Question Answering (EQA) is an important task for modern service robots in various scenarios, which requires efficient exploration in the environment.
                    While prior studies have investigated this problem in a single-agent setting, we envision realistic scenarios with multiple and potentially heterogeneous robots, which are assigned different sets of EQA tasks by the users and allowed to communicate with each other to share information.
                    We refer to this novel setting as a \textit{multi-agent multi-task EQA (MM-EQA)} problem. 
                    To address this challenge, we propose What2Say, a novel LLM-based decentralized communication framework for MM-EQA, where conformal prediction is used to calibrate the generated messages to reduce distractions to other agents due to hallucination and improve communication reliability.
                    To evaluate our framework, we create an MM-EQA benchmark with diverse, photo-realistic household scenarios. 
                    The experimental results demonstrate that our framework enhances both the task success rate and exploration efficiency over baselines by a large margin. 
                </p>
            </div>
        </div>
        
        <!-- TODO -->
        <!-- <div class="row justify-content-md-center">
            <div class="col-md-6 text-center">
                <iframe width="560" height="285" src="" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </div>
        </div> -->

        <!-- <div class="row justify-content-md-center">
            <blockquote class="twitter-tweet"><p lang="en" dir="ltr">LLMs can generate plans and write robot code üìù but they can also make mistakes. How do we get LLMs to ùò¨ùòØùò∞ùò∏ ùò∏ùò©ùò¶ùòØ ùòµùò©ùò¶ùò∫ ùò•ùò∞ùòØ&#39;ùòµ ùò¨ùòØùò∞ùò∏ ü§∑ and ask for help?<br><br>Read more on how we can do this (with statistical guarantees) for LLMs on robots üëá<a href="https://t.co/D7mHGzNP3p">https://t.co/D7mHGzNP3p</a> <a href="https://t.co/M9lUqlZ5cB">pic.twitter.com/M9lUqlZ5cB</a></p>&mdash; Allen Z. Ren (@allenzren) <a href="https://twitter.com/allenzren/status/1677000811803443213?ref_src=twsrc%5Etfw">July 6, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        </div> -->

        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <br>
                <h3>
                    Key Ideas and Contributions
                </h3>
                <p class="text-justify">
                    <br><p style="text-align:center;">
                        <image src="img/Fig2.pdf" width="100%">
                    </p>
                    <strong>1) We introduce a challenging multi-agent embodied cooperation problem, where multiple robots are expected to communicate and complete their individual tasks accurately and efficiently. <br>
                    <strong>2) We develop What2Say, the first decentralized communication framework where messages are calibrated by Conformal Prediction for LLM-based multi-agent cooperation, which generates effective natural language messages to enhance the agents' ability to explore and interact effectively in complex environments. <br>
                    <strong>3) We create an MM-EQA dataset featuring multiple embodied questions in each scenario, which is specifically designed for the multi-agent, multi-task setting.<br>
            </div>
            <br>
        </div>
        <br>
        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <br>
                <h3>
                    Test Results
                </h3>
                <p class="text-justify">
                    <br><p style="text-align:center;">
                        <image src="img/exp1.pdf" width="100%">
                    </p>
                    <strong>Quantitative Analysis</strong>:To better improve the communication efficiency of decentralized LLM-based multi-agent systems, we propose an effective natural language communication framework that employs LLMs for multi-agent cooperation across multiple EQA tasks, which requires each agent to efficiently locate their targets. This framework incorporates conformal prediction to calibrate the confidence of outputs, which filters out overconfident LLM-generated outputs and enables the information in communication effective.
                    Extensive experimental results demonstrate that our method significantly improves exploration efficiency and the overall success rate, particularly in large scenes.  <br>
            </div>
            <br>
        </div>
        <br>

        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <h3>
                    Citation 
                </h3>
                <!-- TODO -->
                <!-- <a href="">[arxiv version]</a> -->
                <div class="form-group col-md-12">
                    <!-- <textarea id="bibtex" class="form-control" rows="6" readonly>
@article{yao2024sonic,
    title={SoNIC: Safe Social Navigation with Adaptive Conformal Inference and Constrained Reinforcement Learning},
    author={Yao, Jianpeng and Zhang, Xiaopan and Xia, Yu and Roy-Chowdhury, Amit K and Li, Jiachen},
    journal={arXiv preprint arXiv:2407.17460},
    year={2024}
}</textarea> -->
                </div>
            </div>
        </div>
        <!-- TODO -->
        <!-- <div class="row justify-content-md-center mt-4">
            <div class="col-md-12 col-lg-10">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                </p>
            </div>
        </div> -->
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script>hljs.highlightAll();</script>
</body>
</html>
